{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# python 3.11\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "1155996d038c88a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset was loaded, and an initial examination of its first few rows was performed to better understand its structure and contents.",
   "id": "43274cb12a45fcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv(\"amazon_reviews.csv\")\n",
    "df.head()"
   ],
   "id": "809647f9dd866bef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A count plot was created to visualize the distribution of star ratings.\n",
    "After displaying the plot, it was observed that the number of reviews was equal for each star rating.x"
   ],
   "id": "4dba89760dbc50a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.countplot(x='Star', data=df)\n",
    "plt.title(\"Distribution of Star Ratings\")\n",
    "plt.xlabel(\"Star Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ],
   "id": "1634fc1c55f7608a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The lengths of the titles and contents were calculated by counting the number of words in each.\n",
    "The average lengths for both titles and contents were then computed to better understand the typical size of the reviews.\n",
    "Afterward, a histogram was created to visualize the distribution of content lengths.\n",
    "From the histogram, it was observed that most reviews contained a relatively small number of words, with only a few reviews being significantly longer."
   ],
   "id": "f6dbf9f3cd7b740e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['title_length'] = df['Title'].astype(str).apply(lambda x: len(x.split()))\n",
    "df['content_length'] = df['Content'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "min_title_length = df['title_length'].min()\n",
    "max_title_length = df['title_length'].max()\n",
    "\n",
    "print(\"Minimum Title Length:\", min_title_length)\n",
    "print(\"Maximum Content Length:\", max_title_length)\n",
    "\n",
    "print(\"Average Title Length:\", df['title_length'].mean())\n",
    "print(\"Average Content Length:\", df['content_length'].mean())\n"
   ],
   "id": "e53cba982d992005"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sns.histplot(df['content_length'], bins=40, kde=True)\n",
    "plt.title(\"Histogram of Content Length\")\n",
    "plt.xlabel(\"Number of Words in Content\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ],
   "id": "6da4a7f4f64ad4ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The text from all reviews was combined into a single string to create a unified body of text.\n",
    "A word cloud was then generated to visually represent the most frequently used words in the review content.\n",
    "After displaying the word cloud, it was observed that words such as \"use,\" \"one,\" and \"product\" appeared most prominently, indicating their frequent usage among the reviews."
   ],
   "id": "2a3a60fb98057d05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "text = \" \".join(review for review in df['Content'].astype(str))\n",
    "wordcloud = WordCloud(width=500, height=350).generate(text)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud, interpolation='hermite')\n",
    "plt.axis('off')\n",
    "plt.title(\"Word Cloud of Review Content\")\n",
    "plt.show()\n"
   ],
   "id": "5c95c5b11103efc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The text in the title and content fields was cleaned by converting all characters to lowercase and removing unwanted symbols, while preserving basic punctuation marks for future analysis.\n",
    "After the cleaning process, the title and content of each review were combined into a single text field to facilitate further processing."
   ],
   "id": "bb461bd3998083b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s.,!?]', '', text)\n",
    "    return text\n",
    "\n",
    "df['Title'] = df['Title'].astype(str).apply(clean_text)\n",
    "df['Content'] = df['Content'].astype(str).apply(clean_text)\n",
    "df['Combined'] = df['Title'] + \" \" + df['Content']\n"
   ],
   "id": "ea8ab68d37f2e187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A tokenization function was defined to split the text into individual words or into sequences of two or three consecutive words, depending on the specified value of n. A token is a single unit of text, such as a word or phrase, and tokenization is the process of breaking text into these smaller meaningful units. This allowed the text to be prepared for further analysis using unigrams (single words), bigrams (pairs of consecutive words), or trigrams (triplets of consecutive words).",
   "id": "af0e7d86bfe6174"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize(text, n=1):\n",
    "    words = text.split()\n",
    "    if n == 1:\n",
    "        return words\n",
    "    elif n == 2:\n",
    "        return [f\"{words[i]} {words[i+1]}\" for i in range(len(words)-1)]\n",
    "    elif n == 3:\n",
    "        return [f\"{words[i]} {words[i+1]} {words[i+2]}\" for i in range(len(words)-2)]\n"
   ],
   "id": "432478fb29e9f4e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A sample text from the combined reviews was selected and tokenized into unigrams, bigrams, and trigrams.\n",
    "Through this process, the structure and behavior of the tokenization function were verified."
   ],
   "id": "9e4984169330151d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample = df['Combined'].iloc[0]\n",
    "print(\"Unigrams:\", tokenize(sample, 1))\n",
    "print(\"Bigrams:\", tokenize(sample, 2))\n",
    "print(\"Trigrams:\", tokenize(sample, 3))\n"
   ],
   "id": "99ebfb3928d0f706"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A stopword is defined as a commonly occurring word—such as \"the,\" \"is,\" \"in,\" or \"and\"—that is typically considered to carry limited semantic value in text analysis. These words are often removed during preprocessing so that attention can be focused on more informative terms.\n",
    "\n",
    "In this context, a predefined list of stopwords was selected (FOUND ON GITHUB https://gist.github.com/sebleier/554280). It included personal pronouns (e.g., \"I\", \"you\", \"they\"), auxiliary verbs (e.g., \"is\", \"was\", \"have\"), articles and conjunctions (e.g., \"the\", \"a\", \"and\", \"but\"), and prepositions (e.g., \"in\", \"on\", \"with\"). These words were chosen because they are frequently used across various texts but generally do not contribute meaningful information relevant to analytical tasks.\n",
    "\n",
    "However, words such as \"not\" and \"nor\" were intentionally not included in the stopword list, as they convey negation, which can significantly affect the sentiment or meaning of a sentence. Retaining such words was considered important for preserving the contextual polarity of the text.\n",
    "\n",
    "To ensure that only significant tokens were retained, the selected stopwords were removed from unigrams, bigrams, and trigrams. This step was taken to improve the clarity and effectiveness of subsequent analysis. To evaluate the effectiveness of the stopword removal process, examples were displayed both before and after the filtering was applied."
   ],
   "id": "455fb2f613a879a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "stopwords = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"now\"}\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "df['clean_tokens'] = df['Combined'].apply(lambda x: remove_stopwords(tokenize(x, 1)))\n",
    "\n",
    "\n",
    "def remove_stopwords_from_bigrams(bigrams):\n",
    "    return [bigram for bigram in bigrams\n",
    "            if all(word not in stopwords for word in bigram.split())]\n",
    "\n",
    "df['clean_bigrams'] = df['Combined'].apply(lambda x: remove_stopwords_from_bigrams(tokenize(x, 2)))\n",
    "\n",
    "\n",
    "def remove_stopwords_from_trigrams(trigrams):\n",
    "    return [trigram for trigram in trigrams\n",
    "            if all(word not in stopwords for word in trigram.split())]\n",
    "\n",
    "df['clean_trigrams'] = df['Combined'].apply(lambda x: remove_stopwords_from_trigrams(tokenize(x, 3)))\n"
   ],
   "id": "72b1c5cd1b0524fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A function was implemented to calculate the frequency of tokens within a collection of texts.\n",
    "Depending on the specified n-gram level, unigrams, bigrams etc.\n",
    "Stopwords were removed to focus on more meaningful terms, and the frequency of each token was counted and stored.\n",
    "This process enabled the identification of the most commonly occurring terms or phrases in the dataset. Stopwords will be explained in next code block."
   ],
   "id": "4e882341d3502fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_token_frequencies(texts, n_gram):\n",
    "    freq = defaultdict(int)\n",
    "\n",
    "    for text in texts:\n",
    "        if n_gram == 1:\n",
    "            tokens = remove_stopwords(tokenize(text, 1))\n",
    "        elif n_gram == 2:\n",
    "            tokens = remove_stopwords_from_bigrams(tokenize(text, 2))\n",
    "        elif n_gram == 3:\n",
    "            tokens = remove_stopwords_from_bigrams(tokenize(text, 3))\n",
    "\n",
    "        for token in tokens:\n",
    "            freq[token] += 1\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "print(\"Unigram Before:\", tokenize(df['Combined'].iloc[0], 1))\n",
    "print(\"Unigram After :\", remove_stopwords(tokenize(df['Combined'].iloc[0], 1)))\n",
    "\n",
    "\n",
    "print(\"Bigram Before:\", tokenize(df['Combined'].iloc[0], 2))\n",
    "print(\"Bigram After :\", remove_stopwords_from_bigrams(tokenize(df['Combined'].iloc[0], 2)))\n",
    "\n",
    "\n",
    "print(\"Trigram Before:\", tokenize(df['Combined'].iloc[0], 3))\n",
    "print(\"Trigram After :\", remove_stopwords_from_trigrams(tokenize(df['Combined'].iloc[0], 3)))"
   ],
   "id": "9672f8a06a078f82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A sentiment label was assigned to each review based on its star rating: reviews with 1 or 2 stars were labeled as negative, and those with 4 or 5 stars as positive. Reviews with a 3-star rating were considered neutral and were excluded to maintain a clear separation between sentiment categories. As a result, rows without a sentiment label were removed from the dataset.\n",
    "\n",
    "To assess the impact of how 3-star reviews are handled, three different labeling strategies were compared: treating 3-star reviews as neutral and excluding them, treating them as positive, and treating them as negative. F1 scores were then calculated for each tokenization level (unigram, bigram, trigram) under these strategies.\n",
    "\n",
    "Results and Inferences:\n",
    "1. When 3-star reviews were treated as neutral (excluded):\n",
    "Unigram F1 score: 0.8802\n",
    "\n",
    "Bigram F1 score: 0.8161\n",
    "\n",
    "Trigram F1 score: 0.5850\n",
    "\n",
    "This approach yielded the highest F1 scores overall, especially for unigrams and bigrams. By removing the inherently ambiguous 3-star reviews, the model was trained on more polarized and clearly defined examples of sentiment, allowing for better differentiation between positive and negative classes. This clearer signal likely improved classification performance.\n",
    "\n",
    "2. When 3-star reviews were treated as positive:\n",
    "Unigram F1 score: 0.7732\n",
    "\n",
    "Bigram F1 score: 0.7199\n",
    "\n",
    "Trigram F1 score: 0.5673\n",
    "\n",
    "Treating 3-star reviews as positive resulted in a noticeable drop in F1 scores across all n-gram levels. Since 3-star reviews often contain mixed or neutral language, grouping them with genuinely positive reviews may have introduced noise, reducing the model's ability to learn distinct positive features.\n",
    "\n",
    "3. When 3-star reviews were treated as negative:\n",
    "Unigram F1 score: 0.8331\n",
    "\n",
    "Bigram F1 score: 0.7756\n",
    "\n",
    "Trigram F1 score: 0.5759\n",
    "\n",
    "This strategy performed better than labeling 3-star reviews as positive but still worse than excluding them. It suggests that 3-star reviews may be slightly more aligned in language or tone with negative reviews than with positive ones, but they still add ambiguity and lower the model's precision.\n",
    "\n",
    "Conclusion:\n",
    "The highest classification performance was achieved when 3-star reviews were excluded. This indicates that their inclusion—regardless of whether they were labeled as positive or negative—introduces ambiguity that weakens the sentiment signal. Therefore, for binary sentiment analysis tasks aiming for clear positive/negative separation, it is advisable to remove neutral reviews from the training data."
   ],
   "id": "8237fe0b48e5cc1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def label_sentiment(star):\n",
    "    if star == 1 or star == 2:\n",
    "        return 0\n",
    "    elif star == 4 or star == 5:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['Label'] = df['Star'].apply(label_sentiment)\n",
    "df = df.dropna(subset=['Label'])"
   ],
   "id": "5ca3c05fa1092d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The title and content fields were re-concatenated and stored in the combined text column to ensure that the most up-to-date, cleaned versions of both fields were reflected in the analysis.",
   "id": "2a8e11481c0522a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.loc[:, 'Combined'] = df['Title'].astype(str) + \" \" + df['Content'].astype(str)",
   "id": "34525e1237982d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The dataset was divided into training and testing subsets using an 80-20 split.",
   "id": "538b064cddaecee2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, stratify=df['Label']\n",
    ")"
   ],
   "id": "1f429a4b40d5b776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Token frequencies were calculated from the training dataset using unigrams. A minimum frequency threshold was applied to filter out infrequent tokens. Tokens that appeared at least twice were retained and stored in a set to form the vocabulary of known tokens for further processing.",
   "id": "1cdb656b1fa47627"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "min_freq = 2\n",
    "token_freq = get_token_frequencies(train_df['Combined'], n_gram=1)\n",
    "\n",
    "known_tokens = set(token for token, count in token_freq.items() if count >= min_freq)\n"
   ],
   "id": "a8b1fdcd78bf771b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Naive Bayes Classification is a probabilistic machine learning method that is used for classification tasks. It is based on Bayes’ Theorem and is built on the assumption that features are conditionally independent given the class label. Probabilities for each class are calculated using training data, and the class with the highest posterior probability is assigned to the input.\n",
    "\n",
    "A custom NaiveBayesClassifier was developed to perform sentiment classification based on various n-gram representations. The model was designed to support unigram, bigram, trigram, and combined n-gram configurations. During training, tokens were extracted from the input texts, filtered using a list of known tokens (when provided), and their frequencies were updated per class using weighted counts. The weights were adjusted based on the associated star ratings to reflect varying confidence levels in sentiment (e.g., 1-star and 5-star reviews were given higher weights).\n",
    "\n",
    "During training, each review was tokenized according to the specified n-gram level (1, 2, or 3). Stopwords were removed, and if a list of known tokens was supplied, tokens outside this list were replaced with the placeholder <UNK>. The vocabulary was updated dynamically based on the encountered tokens.\n",
    "\n",
    "Token counts were stored separately for each sentiment class (positive or negative). In addition to individual token frequencies, the total number of tokens and document counts per class were also recorded. These statistics were used during prediction to compute class-conditional probabilities.\n",
    "\n",
    "To reflect varying confidence in sentiment labels, reviews were weighted based on their star ratings:\n",
    "\n",
    "* 1-star and 5-star reviews received a weight of 1.0, as they represent strong sentiment.\n",
    "\n",
    "* 2-star and 4-star reviews were assigned a lower weight of 0.5, reflecting milder sentiment.\n",
    "\n",
    "These weights influenced how strongly tokens from each review contributed to the class likelihoods during training.\n",
    "\n",
    "During prediction, separate token sets were created for the title and content. The probabilities of each class were computed using log-likelihoods, with weighted contributions from the title and content tokens. Laplace smoothing was applied to handle unseen tokens. The class with the highest log-probability was selected as the final prediction.\n",
    "\n",
    "The classifier was trained separately for unigram, bigram, trigram modes using the training set. Each model was then used to predict the sentiment of reviews in the test set, and the predictions were stored in new columns for evaluation. To control the influence of different text fields, weighted contributions from the title and content were used. The following configurations were tested:\n",
    "\n",
    "Title Weight 0.7, Content Weight 0.3:\n",
    "\n",
    "* Unigram F1 Score: 0.88155\n",
    "\n",
    "* Bigram F1 Score: 0.81512\n",
    "\n",
    "* Trigram F1 Score: 0.58503\n",
    "\n",
    "Title Weight 0.6, Content Weight 0.4:\n",
    "\n",
    "* Unigram F1 Score: 0.88015\n",
    "\n",
    "* Bigram F1 Score: 0.81612\n",
    "\n",
    "* Trigram F1 Score: 0.58503\n",
    "\n",
    "Title Weight 0.5, Content Weight 0.5:\n",
    "\n",
    "* Unigram F1 Score: 0.87644\n",
    "\n",
    "* Bigram F1 Score: 0.81442\n",
    "\n",
    "* Trigram F1 Score: 0.58462\n",
    "\n",
    "Title Weight 0.4, Content Weight 0.6:\n",
    "\n",
    "* Unigram F1 Score: 0.87059\n",
    "\n",
    "* Bigram F1 Score: 0.80928\n",
    "\n",
    "* Trigram F1 Score: 0.58430\n",
    "\n",
    "The results indicate that while title-dominant weights generally produced better performance, the best overall balance across unigram and bigram models was achieved at a title weight of 0.6 and content weight of 0.4.\n",
    "\n",
    "This specific configuration was selected because it offered the highest bigram F1 score (0.8161), and nearly the highest unigram score (0.8802). By emphasizing the title slightly more than the content, the model benefited from sentiment-rich summaries while still incorporating contextual details, resulting in more accurate predictions.\n",
    "\n",
    "Results:\n",
    "\n",
    "Unigram Classifier\n",
    "\n",
    "* The unigram model consistently achieved the highest F1 scores across different experiments. This suggests that individual word presence provides strong discriminative signals for sentiment when appropriately weighted and smoothed. The simplicity and low sparsity of unigrams likely contributed to this performance.\n",
    "\n",
    "Bigram Classifier\n",
    "\n",
    "* The bigram model performed slightly worse than the unigram model, though still competitively. The increased vocabulary size and reduced overlap between reviews may have limited their generalizability.\n",
    "\n",
    "Trigram Classifier\n",
    "\n",
    "* The trigram model exhibited the lowest F1 scores across all settings. Trigrams are highly specific and sparse, meaning most token sequences appear infrequently, even in similar reviews. As a result, class probabilities became less reliable, and the model struggled to generalize to unseen data.\n"
   ],
   "id": "69c242006a6679cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, n_gram=1):\n",
    "        self.n_gram = n_gram\n",
    "        self.class_token_counts = defaultdict(lambda: defaultdict(int))\n",
    "        self.class_total_tokens = defaultdict(int)\n",
    "        self.class_doc_counts = defaultdict(int)\n",
    "        self.vocabulary = set()\n",
    "        self.num_docs = 0\n",
    "        self.known_tokens = None\n",
    "\n",
    "    def train(self, texts, labels, stars):\n",
    "        for text, label, star in zip(texts, labels, stars):\n",
    "            self.class_doc_counts[label] += 1\n",
    "            self.num_docs += 1\n",
    "\n",
    "            weight = 1.0\n",
    "            if label == 0:\n",
    "                weight = 1.0 if star == 1 else 0.5\n",
    "            elif label == 1:\n",
    "                weight = 1.0 if star == 5 else 0.5\n",
    "\n",
    "            if self.n_gram == 1:\n",
    "                tokens = remove_stopwords(tokenize(text, 1))\n",
    "            elif self.n_gram == 2:\n",
    "                tokens = remove_stopwords_from_bigrams(tokenize(text, 2))\n",
    "            elif self.n_gram == 3:\n",
    "                tokens = remove_stopwords_from_trigrams(tokenize(text, 3))\n",
    "\n",
    "\n",
    "            for token in tokens:\n",
    "                if self.known_tokens is not None:\n",
    "                    token = token if token in self.known_tokens else \"<UNK>\"\n",
    "                self.vocabulary.add(token)\n",
    "                self.class_token_counts[label][token] += weight\n",
    "                self.class_total_tokens[label] += weight\n",
    "\n",
    "    def predict(self, title, content):\n",
    "\n",
    "\n",
    "        if self.n_gram == 1:\n",
    "            title_tokens = remove_stopwords(tokenize(title, 1))\n",
    "            content_tokens = remove_stopwords(tokenize(content, 1))\n",
    "        elif self.n_gram == 2:\n",
    "            title_tokens = remove_stopwords_from_bigrams(tokenize(title, 2))\n",
    "            content_tokens = remove_stopwords_from_bigrams(tokenize(content, 2))\n",
    "        elif self.n_gram == 3:\n",
    "            title_tokens = remove_stopwords_from_trigrams(tokenize(title, 3))\n",
    "            content_tokens = remove_stopwords_from_trigrams(tokenize(content, 3))\n",
    "\n",
    "\n",
    "        class_scores = {}\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        for c in self.class_doc_counts:\n",
    "            log_prior = math.log(self.class_doc_counts[c] / self.num_docs)\n",
    "            total_tokens = self.class_total_tokens[c]\n",
    "            title_weight = 0.7\n",
    "            content_weight = 0.3\n",
    "\n",
    "            def token_log_sum(tokens, weight):\n",
    "                return sum(\n",
    "                    weight * math.log(\n",
    "                        (self.class_token_counts[c].get(token if token in self.vocabulary else \"<UNK>\", 0) + 1)\n",
    "                        / (total_tokens + vocab_size)\n",
    "                    )\n",
    "                    for token in tokens\n",
    "                )\n",
    "\n",
    "            log_prob = log_prior + token_log_sum(title_tokens, title_weight) + token_log_sum(content_tokens, content_weight)\n",
    "            class_scores[c] = log_prob\n",
    "\n",
    "\n",
    "        return max(class_scores, key=class_scores.get)\n",
    "\n",
    "\n",
    "\n",
    "#Training\n",
    "nb_unigram = NaiveBayesClassifier(n_gram=1)\n",
    "nb_bigram = NaiveBayesClassifier(n_gram=2)\n",
    "nb_trigram = NaiveBayesClassifier(n_gram=3)\n",
    "\n",
    "nb_unigram.train(train_df['Combined'], train_df['Label'], train_df['Star'])\n",
    "nb_bigram.train(train_df['Combined'], train_df['Label'], train_df['Star'])\n",
    "nb_trigram.train(train_df['Combined'], train_df['Label'], train_df['Star'])\n",
    "\n",
    "\n",
    "test_df['pred_uni'] = test_df.apply(lambda row: nb_unigram.predict(row['Title'], row['Content']), axis=1)\n",
    "test_df['pred_bi'] = test_df.apply(lambda row: nb_bigram.predict(row['Title'], row['Content']), axis=1)\n",
    "test_df['pred_tri'] = test_df.apply(lambda row: nb_trigram.predict(row['Title'], row['Content']), axis=1)"
   ],
   "id": "7b9122df2826678c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Predictions generated by each Naive Bayes model were evaluated using a custom compute_metrics function. For each prediction, true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) were counted. Based on these counts, the following metrics were computed:\n",
    "\n",
    "* Accuracy: The ratio of correctly predicted instances over all predictions.\n",
    "\n",
    "* Precision: The ratio of correctly predicted positive instances over all predicted positives.\n",
    "\n",
    "* Recall: The ratio of correctly predicted positive instances over all actual positives.\n",
    "\n",
    "* F1-score: The harmonic mean of precision and recall.\n",
    "\n",
    "Metrics were calculated for each model individually and then assembled into a comparison table. This table was formatted to display the key performance indicators for each model in a concise and interpretable format."
   ],
   "id": "6cab0a3d7c34de58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            TN += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            FN += 1\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if TP + TN + FP + FN > 0 else 0\n",
    "    precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "    f1 = 2 / (1/precision + 1/recall) if precision + recall > 0 else 0\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP, \"TN\": TN, \"FP\": FP, \"FN\": FN,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "\n",
    "\n",
    "metrics_uni = compute_metrics(test_df['Label'].tolist(), test_df['pred_uni'].tolist())\n",
    "metrics_bi = compute_metrics(test_df['Label'].tolist(), test_df['pred_bi'].tolist())\n",
    "metrics_tri = compute_metrics(test_df['Label'].tolist(), test_df['pred_tri'].tolist())\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Model\": \"Unigram\", **metrics_uni},\n",
    "    {\"Model\": \"Bigram\", **metrics_bi},\n",
    "    {\"Model\": \"Trigram\", **metrics_tri}\n",
    "])\n",
    "\n",
    "results_df = results_df[[\"Model\", \"accuracy\", \"precision\", \"recall\", \"f1_score\"]]\n",
    "results_df"
   ],
   "id": "2da265e9d7471996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The predictions generated by each Naive Bayes model were assessed using a custom compute_metrics function. This function was used to calculate key performance metrics based on the counts of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The following metrics were then derived:\n",
    "\n",
    "* Accuracy: The proportion of correctly predicted instances out of all predictions.\n",
    "\n",
    "* Precision: The ratio of correctly predicted positive instances to the total predicted positives.\n",
    "\n",
    "* Recall: The ratio of correctly predicted positive instances to the total actual positives.\n",
    "\n",
    "F1-score: The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "The metrics were computed individually for each Naive Bayes model (unigram, bigram, trigram, and combined) and were organized into a comparison table. This table was structured to present each model's key performance indicators in a clear and easy-to-understand format. The comparison of the models allowed for an evaluation of their relative strengths and weaknesses in terms of prediction accuracy and reliability."
   ],
   "id": "e9310044452c7006"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def manual_confusion_matrix(y_true, y_pred):\n",
    "    TP = FP = TN = FN = 0\n",
    "\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        if true == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif true == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif true == 0 and pred == 0:\n",
    "            TN += 1\n",
    "        elif true == 1 and pred == 0:\n",
    "            FN += 1\n",
    "\n",
    "    return [[TN, FP],\n",
    "            [FN, TP]]\n",
    "\n",
    "\n",
    "cm_uni = manual_confusion_matrix(test_df['Label'].tolist(), test_df['pred_uni'].tolist())\n",
    "print(\" Unigram Confusion Matrix:\")\n",
    "print(\"[TN FP]\")\n",
    "print(cm_uni[0])\n",
    "print(\"[FN TP]\")\n",
    "print(cm_uni[1])\n",
    "\n",
    "cm_bi = manual_confusion_matrix(test_df['Label'].tolist(), test_df['pred_bi'].tolist())\n",
    "print(\"\\n Bigram Confusion Matrix:\")\n",
    "print(\"[TN FP]\")\n",
    "print(cm_bi[0])\n",
    "print(\"[FN TP]\")\n",
    "print(cm_bi[1])\n",
    "\n",
    "cm_tri = manual_confusion_matrix(test_df['Label'].tolist(), test_df['pred_tri'].tolist())\n",
    "print(\"\\n Trigram Confusion Matrix:\")\n",
    "print(\"[TN FP]\")\n",
    "print(cm_tri[0])\n",
    "print(\"[FN TP]\")\n",
    "print(cm_tri[1])"
   ],
   "id": "13056b44cc6cfe12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
